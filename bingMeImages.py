from bing_image_downloader import downloader
from os import listdir
import os
import shutil
import cv2 
import collections
import glob
import os
import imagehash
from PIL import Image 

#Execute the download queries into bing 
def download(querys,itemsByQuery,folder):
    print(">>Starting the download<<")
    
    dwl_folder=folder+'/download/'
    if os.path.exists(dwl_folder):
        print("Download Folder already exists. Adding files...")
    else:
        os.mkdir(dwl_folder)	
    #Run the download
    for obj in querys:
        downloader.download(obj, limit=itemsByQuery,  output_dir=dwl_folder, adult_filter_off=True, force_replace=False, timeout=60)    
    
    return dwl_folder

#Consolidate the images of the different folders generated by the download function into the cons_folder
def consolidateFolders(folder,dwl_folder):
    print(">>Starting consolidate folders<<")
    cons_folder=folder+'/consolidated/'
    if os.path.exists(cons_folder):
        print("Consolidate Folder already exists. Adding files...")
    else:
        os.mkdir(cons_folder)	

    for folders_lvl1 in listdir(dwl_folder):
        for image in listdir(dwl_folder+ folders_lvl1):
            img_path = dwl_folder + folders_lvl1+'/'+image
            shutil.move(img_path, cons_folder+folders_lvl1.replace(" ", "")+image)
    return cons_folder

#Resize all the images of a folder and copy them to resize_folder
def resizeImages(folder,cons_folder,size,ext):
    print(">>Starting resize images<<")
    resize_folder=folder+'/'+str(size)+'/'
    if os.path.exists(resize_folder):
        print("Resize Folder already exists. Adding files...")
    else:
        os.mkdir(resize_folder)	
    #shutil.rmtree(resize_folder)
   
    for image in listdir(cons_folder):
        if image.endswith(ext):
            img = cv2.imread(cons_folder+image,1)
            print(cons_folder+image)
            img = cv2.resize(img, (size, size))
            cv2.imwrite(resize_folder+image,img,[cv2.IMWRITE_JPEG_QUALITY, 100]) 
    return resize_folder

#Remove the duplicated files on a folder and extension 
def removeDuplicates(folder, ext):
    print(">>Starting remove duplicates<<")

    hashes = collections.defaultdict(list)
    for fpath in glob.glob(os.path.join(folder, "*.{}".format(ext))):
        h = imagehash.average_hash(Image.open(fpath))
        hashes[h].append(fpath)

    for h,fpaths in hashes.items():
        if len(fpaths) == 1:
            #This file is unique
            continue
        print("The following files are duplicates of each other (with the hash {}): \n\t{}".format(h, '\n\t'.join(fpaths)))
        fpaths.pop(0) #remove one item, we want to keep the original one
        for f in fpaths:
            try:
                os.remove(f)
            except OSError:
                pass

#Creates the dataset folder and start the whole process
def createDataset(querys,itemsByQuery,folder,size):
    if os.path.exists(folder):
        print("Folder already exists")
    else:
        os.mkdir(folder)	
    dwl_folder    = download(querys,itemsByQuery,folder)
    cons_folder   = consolidateFolders(folder,dwl_folder)
    resize_folder = resizeImages(folder,cons_folder,size,'jpg')
    removeDuplicates(resize_folder,'jpg')
    
#Example of how to generate queries
my_objects=[]
for num in range(1, 111,):
    my_objects.append('MESSIER '+str(num))
for num in range(210, 7840,):
    my_objects.append('NGC '+str(num)+' space')

#Or simply create something like:
my_objects=['hubble images']

#And then finally execute all the process with something like:
# my_objects: the queries we want to send to Bing API
# 200: the number of images we want to download for each query
# 160: we want the images to be resized to 160x160 pixels
createDataset(my_objects,200,"./ufo",160)